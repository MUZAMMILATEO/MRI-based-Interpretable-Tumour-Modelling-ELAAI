# Explainable and Likelihood-Aware AI (ELAAI) Framework

[![DOI](https://zenodo.org/badge/912808784.svg)](https://doi.org/10.5281/zenodo.15624608)

This repository implements the **Explainable and Likelihood-Aware AI (ELAAI)** framework, featuring:

- **MFA-Net**: Multi-scale Feature Aggregation network for robust bladder segmentation
- **Refinement Step**: A two-phase strategy to enhance MFA-Net predicted masks
- **SLIP-Net**: Swin-Transformer backbone with MSDU head for pixel-level tumour-likelihood estimation  

## ğŸ§© Methodology (left) & Architectures (right)

<p align="center">
  <img src="media/media_01.png" alt="ELAAI Framework Overview" width="45%"/>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <!-- 14 spaces -->
  <img src="media/media_02.png" alt="ELAAI Model Architectures" width="45%"/>
</p>

## ğŸ“Š Results: Bladder Segmentation Comparisons

The figure below shows MRI-based bladder segmentation performance, highlighting the improvements achieved with the ELAAI framework compared to baseline methods.

<p align="center">
  <img src="media/media_03.png" alt="MRI-based bladder segmentation with comparisons" width="85%"/>
</p>

## ğŸ¯ Results: Tumour Delineation Generalisation

Remarkably, the ELAAI framework improves tumour boundary delineation **even when trained solely on normal bladder data**, demonstrating strong generalisation capabilities.

<p align="center">
  <img src="media/media_04.png" alt="Improved tumour delineation despite training only on normal bladder data" width="85%"/>
</p>

## ğŸ” Results: Tumour Likelihood Maps

The figure below presents tumour likelihood maps generated by **SLIP-Net** and other state-of-the-art frameworks, overlaid on MRI scans. Notably, these maps were produced even though SLIP-Net was trained **only on normal bladder data**, showcasing the frameworkâ€™s potential for **out-of-distribution generalisation**.

<p align="center">
  <img src="media/media_05.png" alt="Tumour likelihood maps overlaid on MRIs" width="85%"/>
</p>

---

---


<!--
## ğŸ“‚ Model Checkpoints

Pre-trained weights are available for both networks. Please download and place them in the following directories before running experiments:

| Model     | Download Link                                                                                                    | Destination Path                                                                                                  |
|-----------|------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|
| MFA-Net   | [Google Drive](https://drive.google.com/file/d/18_hG6FWS_Wf7qxi9CvRPujajGLEQkrPK/view?usp=drive_link)              | `./MFA_Net/ModelSaveTensorFlow/MFANet.h5`                                      |
| SLIP-Net  | [Google Drive](https://drive.google.com/file/d/1DfUrKidnrZwmjbPDDx-WmttUZyBNX_Mq/view?usp=drive_link)              | `./SLIP_Net/experiments/SLIP_ssim_1_diffusion_1/dsc0.999.pth.tar`                                                 |

---

-->

## ğŸ“¦ Code Archive & Citation

The code and data underpinning our publication have been permanently archived and can be referenced via the following DOI badge:

[![DOI](https://zenodo.org/badge/912808784.svg)](https://doi.org/10.5281/zenodo.15624608)

Please cite this DOI in any attribution or reproduction of our work:

---

## ğŸ“¬ Correspondence

For questions, issues or collaboration inquiries, please contact:

**Dr Muzammil Khan**  
Robotics & Mechatronics Group, University of Twente  
âœ‰ï¸ [m.khan@utwente.nl](mailto:m.khan@utwente.nl)  

